{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classifier-Gene-2.ipynb","provenance":[{"file_id":"1U9ftEjvAhmuMC3TAxOtrxCS198jkxIOI","timestamp":1603116992201}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"61CxSPpSoIjm"},"source":["##Mounting drive"]},{"cell_type":"code","metadata":{"id":"67qUOy8hmg1g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622887956593,"user_tz":-360,"elapsed":448,"user":{"displayName":"Meheraj Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiAuvpyWLd_4KOrZZRPrBd0BagXYIQhgtFl3cRDhQ=s64","userId":"08278156670649380800"}},"outputId":"3fdc2d91-c759-4b9b-f796-474768a05ede"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VSI9XhZioM7T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622887958903,"user_tz":-360,"elapsed":594,"user":{"displayName":"Meheraj Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiAuvpyWLd_4KOrZZRPrBd0BagXYIQhgtFl3cRDhQ=s64","userId":"08278156670649380800"}},"outputId":"9d38b515-a3ec-41f9-9b1e-2ebc70028e61"},"source":["%cd /content/drive/My\\ Drive/Thesis/Notebooks\n","%ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Thesis/Notebooks\n"," Classifier-Gene-2.ipynb\n"," Classifier-Gene-2_kernel_width_test.ipynb\n"," Classifier-Gene.ipynb\n"," Classifier-Normal-2.ipynb\n"," Classifier-Normal-2-Kernel_width-Testing.ipynb\n"," Classifier-Normal.ipynb\n","'Copy1 of Classifier-Normal-2.ipynb'\n","'Copy2 of Classifier-Normal-2.ipynb'\n","'Copy3 of Classifier-Normal-2.ipynb'\n"," IRelief2.ipynb\n"," IRelief.ipynb\n"," Load_dataset.ipynb\n"," Modified_MultiSurf.ipynb\n"," Modified_Overlapping_MultiSurf.ipynb\n"," MultiSurf.ipynb\n"," Ovelapping_ReliefF.ipynb\n"," Overlapping_MultiSurf.ipynb\n"," ReliefF.ipynb\n"," Relief.ipynb\n"," RFS.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7zYeVNrGoFJe"},"source":["##Installing Libraries"]},{"cell_type":"code","metadata":{"id":"gSgNb7Jwl0Qj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622887966140,"user_tz":-360,"elapsed":5271,"user":{"displayName":"Meheraj Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiAuvpyWLd_4KOrZZRPrBd0BagXYIQhgtFl3cRDhQ=s64","userId":"08278156670649380800"}},"outputId":"4f4fd6c4-836c-49ed-f263-fbcb92ca0fba"},"source":["!pip install ipynb\n","!pip install skrebate"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting ipynb\n","  Using cached https://files.pythonhosted.org/packages/31/42/4c0bbb66390e3a68e04ebf134c8d074a00c18b5882293f8ace5f7497fbf0/ipynb-0.5.1-py3-none-any.whl\n","Installing collected packages: ipynb\n","Successfully installed ipynb-0.5.1\n","Requirement already satisfied: skrebate in /usr/local/lib/python3.7/dist-packages (0.62)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from skrebate) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from skrebate) (0.22.2.post1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from skrebate) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->skrebate) (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-dP4QJB1JUWU"},"source":["##Importing Libraries"]},{"cell_type":"code","metadata":{"id":"TZtXi4XtjVdt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622887974018,"user_tz":-360,"elapsed":6271,"user":{"displayName":"Meheraj Hossain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiAuvpyWLd_4KOrZZRPrBd0BagXYIQhgtFl3cRDhQ=s64","userId":"08278156670649380800"}},"outputId":"56fb3b70-e9d8-4743-b200-1d0fc9ec679e"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split, RepeatedKFold, RepeatedStratifiedKFold, StratifiedKFold, KFold, LeaveOneOut\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n","from sklearn.svm import SVC\n","from sklearn.pipeline import make_pipeline\n","from skrebate import ReliefF, MultiSURF, SURF, SURFstar, MultiSURFstar\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_score\n","from ipynb.fs.full.Load_dataset import load_dataset, get_dataset_names, load_gene_dataset, get_gene_dataset_names\n","from ipynb.fs.full.ReliefF import ReliefF_configure\n","from ipynb.fs.full.MultiSurf import MultiSurf_configure\n","from ipynb.fs.full.Modified_MultiSurf import Modified_MultiSurf_configure\n","from ipynb.fs.full.RFS import RFS_configure\n","from ipynb.fs.full.Relief import Relief_configure\n","from ipynb.fs.full.IRelief import I_Relief_configure\n","from ipynb.fs.full.IRelief2 import I_Relief2_configure\n","from ipynb.fs.full.Overlapping_MultiSurf import Overlapping_MultiSurf_configure\n","from ipynb.fs.full.Modified_Overlapping_MultiSurf import Modified_Overlapping_MultiSurf_configure\n","from ipynb.fs.full.Ovelapping_ReliefF import Ovelapping_ReliefF_configure\n","from os import path\n","import time"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","dataset: waveform\n","instances = 5000, features= 21 \n","[0 1] [0.4        0.11818182]\n","[1 0] [-0.12702534 -0.1031529 ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fTS3ES2oZwHf"},"source":["folderPath = '/content/drive/My Drive/Thesis/Results'\n","randomState = 123\n","plt.rc('xtick',labelsize=15)\n","plt.rc('ytick',labelsize=15)\n","\n","plotStyles = {\n","      'markers' : [\"o\",\"^\",\"*\",\"s\",\"P\",\"X\",\"d\", \"p\", \"H\",\"D\"],\n","      'color' : ['#273c75', '#6c5ce7', '#d63031', '#00b894','#00cec9',  '#0984e3', '#74b9ff', '#6ab04c', '#e056fd', '#f9ca24'],\n","      'lineStyle' : 'solid',\n","      'markerSize' :  10,\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xwc8JU6LJZQN"},"source":["## Load Dataset"]},{"cell_type":"code","metadata":{"id":"oER4OOrSJbzp"},"source":["def dataset_preparetion(iterationNum,dName, dataset_dict, noisePercentage = 0, datasetType= 'normal'):\n","  global randomState, folderPath\n","  print('datasetType= ',datasetType)\n","  X = dataset_dict['attributes']\n","  Y = dataset_dict['target']\n","  categoricalX = dataset_dict['categoricalX']\n","  trainSize = dataset_dict['trainSize']\n","  testSize = dataset_dict['testSize']\n","  # print(\"Before : \", X.shape, Y.shape)\n","\n","  # Adding irrelavant Features\n","  mu, sigma = 0, 5 # mean and standard deviation\n","  np.random.seed(randomState)\n","  irrelevantFeatures = np.random.normal(mu, sigma, (X.shape[0], 50))\n","  X = np.concatenate((X, irrelevantFeatures), axis=1)\n","  # print(\"After : \", X.shape, Y.shape)  \n","  \n","  XTrain, XTest, YTrain, YTest = train_test_split(X, Y, train_size = trainSize, test_size= testSize, stratify=Y, random_state = randomState + iterationNum)\n","\n","  # Adding noise i.e. misslabeling training data\n","  if noisePercentage > 0:\n","    classes = np.unique(YTrain)\n","\n","    numOfMislabels = np.floor(len(YTrain) * (noisePercentage/100)).astype(int)\n","    # print(numOfMislabels)\n","\n","    # Choose the random instances\n","    rndIdx = np.random.choice(len(YTrain), numOfMislabels, replace=False)   # random indices of m instances from total of n instances\n","    # print(rndIdx)\n","\n","    for idx in rndIdx:\n","      restOfTheClasses = classes[classes != YTrain[idx]]\n","      YTrain[idx] = restOfTheClasses[np.random.choice(len(restOfTheClasses), 1, replace=False).item()]\n","      # print(\"before : \", Y[idx])\n","      # print('after : ', newLabel) \n","\n","  return XTrain, XTest, YTrain, YTest, categoricalX"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aspI2yNAOwWV"},"source":["##Preprocessing and Feature Selection"]},{"cell_type":"code","metadata":{"id":"nlUuInzD2S6-"},"source":["def feature_selection(X, Y, methods, K_RFS = 7, K_ReliefF = 10, theta_RFS = 3, numUpdates='all', numOfIterations= 5, prior='uniform', categoricalX='off', kernelWidth = 5, theta_IRelief = 1e-9): \n","  cache = {}\n","  if 'RFS' in methods:\n","    cache['RFS'] = RFS_configure(X,Y, K = K_RFS, theta= theta_RFS)\n","  \n","  isBinary = len(np.unique(Y)) == 2\n","  if 'Relief' in methods:\n","    if isBinary:\n","       cache['Relief'] = Relief_configure(X,Y, numUpdates=numUpdates, categoricalX=categoricalX)\n","  \n","  if 'ReliefF' in methods:\n","    start = time.time()\n","    cache['ReliefF'] = ReliefF_configure(X, Y, K=K_ReliefF, prior=prior, numUpdates=numUpdates, categoricalX=categoricalX) \n","    print('ReliefF run time= ',time.time()-start)\n"," \n","  if 'IRelief' in methods:\n","    start = time.time()\n","    cache['IRelief'] = I_Relief_configure(X, Y, numOfIterations=numOfIterations, categoricalX = categoricalX, kernelWidth = kernelWidth, theta = theta_IRelief, prior=prior)\n","    print('IRelief run time= ',time.time()-start)\n","\n","  if 'IRelief2' in methods:\n","    start = time.time()\n","    cache['IRelief2'] = I_Relief2_configure(X, Y, numOfIterations=numOfIterations, categoricalX = categoricalX, kernelWidth = kernelWidth, theta = theta_IRelief)\n","    print('IRelief2 run time= ',time.time()-start)\n","\n","  if 'MultiSurf' in methods:\n","    start = time.time()\n","    cache['MultiSurf'] = MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX)\n","    print('MultiSurf run time= ',time.time()-start)\n","\n","  if 'Modified_MultiSurf' in methods:\n","    start = time.time()\n","    cache['Modified_MultiSurf'] = Modified_MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth = kernelWidth)\n","    print('Modified_MultiSurf run time= ',time.time()-start)\n","\n","  if 'Overlapping_MultiSurf' in methods:\n","    start = time.time()\n","    cache['Overlapping_MultiSurf'] = Overlapping_MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth = kernelWidth)\n","    print('Overlapping_MultiSurf run time= ',time.time()-start)\n","\n","  if 'Modified_Overlapping_MultiSurf' in methods:\n","    start = time.time()\n","    cache['Modified_Overlapping_MultiSurf'] = Modified_Overlapping_MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth = kernelWidth)\n","    print('Modified_Overlapping_MultiSurf run time= ',time.time()-start)\n","\n","  # for kernel widh choose **********************************************************************\n","  if 'Modified_Overlapping_MultiSurf_0.05' in methods:\n","    start = time.time()\n","    cache['Modified_Overlapping_MultiSurf_0.05'] = Modified_Overlapping_MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth = 0.05)\n","    print('Modified_Overlapping_MultiSurf_0.05 run time= ',time.time()-start)\n","\n","  if 'Modified_Overlapping_MultiSurf_0.3' in methods:\n","    start = time.time()\n","    cache['Modified_Overlapping_MultiSurf_0.3'] = Modified_Overlapping_MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth = 0.3)\n","    print('Modified_Overlapping_MultiSurf_0.3 run time= ',time.time()-start)\n","\n","  if 'Modified_Overlapping_MultiSurf_1' in methods:\n","    start = time.time()\n","    cache['Modified_Overlapping_MultiSurf_1'] = Modified_Overlapping_MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth = 1)\n","    print('Modified_Overlapping_MultiSurf_1 run time= ',time.time()-start)\n","\n","  if 'Modified_Overlapping_MultiSurf_3' in methods:\n","    start = time.time()\n","    cache['Modified_Overlapping_MultiSurf_3'] = Modified_Overlapping_MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth = 3)\n","    print('Modified_Overlapping_MultiSurf_3 run time= ',time.time()-start)\n","\n","  if 'Modified_Overlapping_MultiSurf_5' in methods:\n","    start = time.time()\n","    cache['Modified_Overlapping_MultiSurf_5'] = Modified_Overlapping_MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth = 5)\n","    print('Modified_Overlapping_MultiSurf_5 run time= ',time.time()-start)\n","\n","  if 'Modified_Overlapping_MultiSurf_10' in methods:\n","    start = time.time()\n","    cache['Modified_Overlapping_MultiSurf_10'] = Modified_Overlapping_MultiSurf_configure(X,Y,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth = 10)\n","    print('Modified_Overlapping_MultiSurf_10 run time= ',time.time()-start)\n","\n","  # *********************************************************************************************\n","  if 'Overlapping_ReliefF' in methods:\n","    start = time.time()\n","    cache['Overlapping_ReliefF'] = Ovelapping_ReliefF_configure(X,Y,K=K_ReliefF,prior=prior, numUpdates=numUpdates, categoricalX=categoricalX, kernelWidth=kernelWidth)\n","    print('Overlapping_ReliefF run time= ',time.time()-start)\n","\n","  if 'ReBATE_ReliefF' in methods:\n","    start = time.time()\n","    reliefF = ReliefF(n_features_to_select=min(X.shape[1], 1000), n_neighbors=K_ReliefF)\n","    reliefF.fit(X, Y)\n","    weight = reliefF.feature_importances_\n","    ranked = reliefF.top_features_\n","    cache['ReBATE_ReliefF'] = ranked, weight\n","    print('ReBATE_ReliefF run time= ',time.time()-start)\n","\n","  if 'ReBATE_SURF' in methods:\n","    start = time.time()\n","    surf = SURF(n_features_to_select=min(X.shape[1], 1000))\n","    surf.fit(X, Y)\n","    weight = surf.feature_importances_\n","    ranked = surf.top_features_\n","    cache['ReBATE_SURF'] = ranked, weight\n","    print('ReBATE_SURF run time= ',time.time()-start)\n","\n","  if 'ReBATE_SURFstar' in methods:\n","    start = time.time()\n","    surfStar = SURFstar(n_features_to_select=min(X.shape[1], 1000))\n","    surfStar.fit(X, Y)\n","    weight = surfStar.feature_importances_\n","    ranked = surfStar.top_features_\n","    cache['ReBATE_SURFstar'] = ranked, weight\n","    print('ReBATE_SURFstar run time= ',time.time()-start)\n","\n","  if 'ReBATE_MultiSURFstar' in methods:\n","    start = time.time()\n","    multiSurfStar = MultiSURFstar(n_features_to_select=min(X.shape[1], 1000))\n","    multiSurfStar.fit(X, Y)\n","    weight = multiSurfStar.feature_importances_\n","    ranked = multiSurfStar.top_features_\n","    cache['ReBATE_MultiSURFstar'] = ranked, weight\n","    print('ReBATE_MultiSURFstar run time= ',time.time()-start)\n","\n","  if 'ReBATE_MultiSURF' in methods:\n","    start = time.time()\n","    multiSurf = MultiSURF(n_features_to_select=min(X.shape[1], 1000))\n","    multiSurf.fit(X, Y)\n","    weight = multiSurf.feature_importances_\n","    ranked = multiSurf.top_features_\n","    cache['ReBATE_MultiSURF'] = ranked, weight\n","    print('ReBATE_MultiSURF run time= ',time.time()-start)  \n","\n","  return cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6RGgLKSkAvO"},"source":["# print('ranked')\n","# print(ranked)\n","# print('Rvalue')\n","# print(Rvalue[np.array(ranked, dtype=int)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmCMpUnMlKvB"},"source":["# print('ranked')\n","# print(reliefF_ranked)\n","# print('reliefF_weight')\n","# print(reliefF_weight[np.array(reliefF_ranked, dtype=int)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RsHQEx8IJRXs"},"source":["##Feature Scaling"]},{"cell_type":"code","metadata":{"id":"BVwzeoaYJZDF"},"source":["def feature_scaling(XTrain, XTest):\n","  scaler = StandardScaler()\n","  scaler.fit(XTrain)\n","  # print(\"Mean :\",scaler.mean_, \"Var :\", scaler.var_)\n","\n","  XTrain = scaler.transform(XTrain)\n","  XTest = scaler.transform(XTest)\n","\n","  return XTrain, XTest, scaler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xqQHsAV6MtHJ"},"source":["##KNN classification"]},{"cell_type":"code","metadata":{"id":"SV5ytnyPMwuu"},"source":["def KNN_classifier(XTrain, YTrain, XTest, YTest, Num_neighbors=7):\n","  classifier = KNeighborsClassifier(n_neighbors=Num_neighbors)\n","  classifier.fit(XTrain, YTrain)  \n","  Y_pred = classifier.predict(XTest)\n","  score = accuracy_score(YTest, Y_pred)\n","  # score1 = classifier.score(XTest, YTest)\n","  # print(score,score1)\n","  \n","  return Y_pred, score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_j8wcd5nMxIo"},"source":["##SVM classification"]},{"cell_type":"code","metadata":{"id":"qQ1WRzIVMxiK"},"source":["def SVM_classifier(XTrain, YTrain, XTest, YTest, kernel='linear', degree=3):\n","  # degree is for polynomial kernel\n","  svclassifier = SVC(kernel=kernel)\n","  svclassifier.fit(XTrain, YTrain)\n","  Y_pred = svclassifier.predict(XTest)\n","  score = accuracy_score(YTest, Y_pred)\n","\n","  return Y_pred, score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lbaXBPvTkUW"},"source":["##Evaluataion"]},{"cell_type":"code","metadata":{"id":"GalU5qlib80U"},"source":["def evaluation(YTest, Y_pred):\n","  print(confusion_matrix(YTest, Y_pred))\n","  print(classification_report(YTest, Y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"byvuWJXlaSoP"},"source":["##Classification"]},{"cell_type":"code","metadata":{"id":"03M6RvLNbSAg"},"source":["def classification(XTrain, XTest, YTrain, YTest, ranked, algo, featureRange, datasetName, Num_neighbors=7, kernel='linear', degree=3):\n","\n","  scoreCollection = np.array([])\n","  YPredCollection = []\n","  XTrain, XTest, _ = feature_scaling(XTrain, XTest)\n","\n","  for n in featureRange:\n","    feature_idx = np.array(ranked[0:n])\n","    \n","    # if np.sum(feature_idx>=XTrain.shape[1]-50):\n","    #   print('irrelevent selected: ',feature_idx)\n","    # print(feature_idx)\n","    \n","    # print(feature_idx)\n","    scores = np.array([])\n","    XSubTrain, XSubTest = XTrain[:, feature_idx], XTest[:, feature_idx]\n","    \n","    # print(\"XSubTRAIN:\", XSubTrain.shape, \"XSubTEST:\", XSubTest.shape)\n","    # print(\"YTRAIN:\", YTrain.shape, \"YTEST:\", YTest.shape)\n","\n","    if algo == 'KNN':\n","      YPred, score = KNN_classifier(XSubTrain, YTrain, XSubTest, YTest,  Num_neighbors=Num_neighbors)\n","    else:\n","      YPred, score = SVM_classifier(XSubTrain, YTrain, XSubTest, YTest, kernel=kernel, degree=degree)\n","\n","    scoreCollection = np.append(scoreCollection, score)\n","    YPredCollection.append(YPred)\n","\n","  # print(datasetName, scoreCollection)\n","  return np.array(YPredCollection), scoreCollection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xo4sY8jziQEd"},"source":["## Run and Save results"]},{"cell_type":"code","metadata":{"id":"6SYTjVIF30es"},"source":["def performanceFileWrite(metricName, arrayCollection, index, featureRange, dName, classifierName, isNoisy ): \n","  global folderPath \n","  filePath = \"\"\n","  if isNoisy:\n","    filePath = folderPath  +  '/Gene/Performance/{}/{}_{}_withNoise.csv'.format(metricName, dName, classifierName)\n","  else:\n","    filePath = folderPath  +  '/Gene/Performance/{}/{}_{}_withoutNoise.csv'.format(metricName, dName, classifierName)\n","  if path.exists(filePath):\n","    df = pd.read_csv(filePath, sep=\",\", index_col=['Method', 'Classes'])\n","    for i in range(index.to_series().nunique()):\n","      df.loc[index[i],:] = arrayCollection[i] \n","  else:\n","    df = pd.DataFrame(arrayCollection, index=index, columns=featureRange)\n","    df = df.rename_axis('#features', axis='columns')\n"," \n","  print(\"{} for the dataset {} and classifier {} (Noisy = {})\".format(metricName ,dName, classifierName, isNoisy))\n","  print(df)\n","  df.to_csv(filePath, sep=\",\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uga-St7HeSGT"},"source":["def performanceReport(methods, YActual, YPredDict, featureRange, dName, isNoisy, classifierName):\n","  \n","  truePosPlusSupportArrayCollection = []\n","  precisionArrayCollection, recallArrayCollection, f1scoreArrayCollection = [], [], []\n","  for method in methods: \n","    truePosPlusSupportDict = {}\n","    precisionDict, recallDict, f1scoreDict = {}, {}, {}\n","    for YPred in YPredDict[method]:\n","      confMat = confusion_matrix(YActual, YPred)\n","      precision, recall, f1score, _ = precision_recall_fscore_support(YActual, YPred, average=None)\n","      truePositives = np.diag(confMat) \n","      support = np.sum(confMat, axis = 1) \n","      for i in range(len(truePositives)):\n","        if i not in truePosPlusSupportDict:\n","          truePosPlusSupportDict[i] = np.array(['A={},TP={}'.format(support[i], truePositives[i])])\n","          precisionDict[i] = np.array(precision[i])\n","          recallDict[i] = np.array(recall[i])\n","          f1scoreDict[i] = np.array(f1score[i])\n","        else:\n","          truePosPlusSupportDict[i] = np.append(truePosPlusSupportDict[i], 'A={},TP={}'.format(support[i], truePositives[i]))\n","          precisionDict[i] = np.append(precisionDict[i], precision[i])\n","          recallDict[i] = np.append(recallDict[i], recall[i])\n","          f1scoreDict[i] = np.append(f1scoreDict[i], f1score[i])\n","\n","    truePosPlusSupportArray = []\n","    precisionArray, recallArray, f1scoreArray = [], [], []\n","    for key in truePosPlusSupportDict:\n","      truePosPlusSupportArray.append(truePosPlusSupportDict[key])\n","      precisionArray.append(precisionDict[key])\n","      recallArray.append(recallDict[key])\n","      f1scoreArray.append(f1scoreDict[key])\n","    truePosPlusSupportArrayCollection = truePosPlusSupportArrayCollection + truePosPlusSupportArray\n","    precisionArrayCollection = precisionArrayCollection + precisionArray\n","    recallArrayCollection = recallArrayCollection + recallArray\n","    f1scoreArrayCollection = f1scoreArrayCollection + f1scoreArray\n","\n","  truePosPlusSupportArrayCollection = np.array(truePosPlusSupportArrayCollection)\n","  precisionArrayCollection = np.array(precisionArrayCollection)\n","  recallArrayCollection = np.array(recallArrayCollection)\n","  f1scoreArrayCollection = np.array(f1scoreArrayCollection)\n","  \n","  classes = np.unique(YActual)\n","  # Confusion matrix\n","  index = pd.MultiIndex.from_product([methods, classes], names=['Method', 'Classes'])\n","\n","  performanceFileWrite('ConfusionMatrix', truePosPlusSupportArrayCollection, index, featureRange, dName, classifierName, isNoisy )\n","  performanceFileWrite('Precision', precisionArrayCollection, index, featureRange, dName, classifierName, isNoisy )\n","  performanceFileWrite('Recall', recallArrayCollection, index, featureRange, dName, classifierName, isNoisy )\n","  performanceFileWrite('F1score', f1scoreArrayCollection, index, featureRange, dName, classifierName, isNoisy )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FfZGe5Gm8pQk"},"source":["### Feature weight calculating"]},{"cell_type":"code","metadata":{"id":"a_Mtu0UM7rdF"},"source":["def run_and_save_feature_weights(methods, dName, datasetType='gene', numOfFolds = 10, numOfRepeats = 5, noisePercentage=0, numOfPoints = 30, firstEndPoint = 500, secondEndPoint = 2000):\n","  global randomState, folderPath\n","\n","  # print('in run_and_save_output datasetType= ',datasetType)\n","  isNoisy = noisePercentage > 0\n","\n","  dataset_dict =   load_dataset(dName) if (datasetType == 'normal') else load_gene_dataset(dName) \n","  # print('datasetType= ',datasetType)\n","\n","  X = dataset_dict['attributes']\n","  Y = dataset_dict['target']\n","  categoricalX = dataset_dict['categoricalX'] \n","  featureNames = dataset_dict['featureNames']\n","  \n","  # leaveOneOut = LeaveOneOut()\n","  # numOfsplits = leaveOneOut.get_n_splits(X)\n","  repeatedKFold = RepeatedKFold(n_splits = numOfFolds, n_repeats = numOfRepeats, random_state=randomState)\n","  numOfsplits = repeatedKFold.get_n_splits(X)\n","\n","  weightsOfMethods = {}\n","  ranksOfMethods = {}\n","\n"," \n","  minItrSaved = 1e9\n","  dfPrevWeights, dfPrevRanks= {}, {}\n","  # Load the pre saved iteration for each methods\n","  for method in methods:\n","    weigthFileExists = path.exists(folderPath  +  '/Gene/Weights/{}/{}.csv'.format(dName, method))\n","    rankFileExists = path.exists(folderPath  +  '/Gene/Ranks/{}/{}.csv'.format(dName, method))\n","    if weigthFileExists:\n","      dfPrevWeights[method] = pd.read_csv(folderPath  +  '/Gene/Weights/{}/{}.csv'.format(dName, method), sep=\",\",index_col=\"Iterations\", header=0)\n","      minItrSaved = min(minItrSaved, dfPrevWeights[method].index.shape[0]-1)\n","    else:\n","      minItrSaved = 0\n","\n","    if rankFileExists:\n","      dfPrevRanks[method] = pd.read_csv(folderPath  +  '/Gene/Ranks/{}/{}.csv'.format(dName, method), sep=\",\",index_col=\"Iterations\", header=0)\n","      minItrSaved = min(minItrSaved, dfPrevRanks[method].index.shape[0]-1)\n","    else:\n","      minItrSaved = 0\n","\n","  iterationNo = -1\n","  for trainIndex, testIndex in repeatedKFold.split(X):\n","    iterationNo+=1\n","    # print(\"TRAIN:\", trainIndex, \"TEST:\", testIndex)\n","    XTrain, XTest, YTrain, YTest = X[trainIndex], X[testIndex], Y[trainIndex], Y[testIndex]\n","\n","    # Here if we estimate best K for KNN classification using 10 fold stratified cross validation, it will help us to do comparison more fairly.\n","    # Here If we estimate best kernelWidth sigma for I-Relief using 10 fold stratified cross validation, it will help us to do comparison more fairly. \n","    # Here If we estimate best K(i.e. number of nearest hits and misses) for Relief-F using 10 fold stratified cross validation, it will help us to do comparison more fairly. \n","\n","\n","    # performing feture selection \n","    # Using kernelwidth sigma = 3 and number of iterations = 20 and convergence_threshold = 1e-5 for I-Relief-1 and I-Relief-2\n","    # Using number of nearest hits and misses i.e. K = 10 for ReliefF and also using all the training data for Relief and ReliefF\n","    # Using number of nearest Neighbours k = 7 and overlapping_threshold = 3 for RFS\n","    # print('Training data size: training examples = {}, features= {}'.format(XTrain.shape[0], XTrain.shape[1]))\n","    \n","    cache ={}\n","    if iterationNo>=minItrSaved:\n","      # Feature Selection\n","      cache = feature_selection(XTrain, YTrain, methods, K_RFS = 7, K_ReliefF = 10, theta_RFS = 3, numUpdates='all', numOfIterations = 20, prior='empirical', categoricalX='off', kernelWidth = 5, theta_IRelief = 1e-5)\n","  \n","    for method in methods:\n","      ranked, weight= None, None\n","      if iterationNo>=minItrSaved:\n","        ranked, weight = cache[method]        \n","      else:\n","        ranked, weight = dfPrevRanks[method].loc[iterationNo,:].to_numpy(), dfPrevWeights[method].loc[iterationNo,:].to_numpy()\n","\n","      if method not in weightsOfMethods:\n","        weightsOfMethods[method] = [weight]\n","        ranksOfMethods[method] = [ranked]\n","      else:\n","        weightsOfMethods[method].append(weight)\n","        ranksOfMethods[method].append(ranked)\n","\n","      if iterationNo == minItrSaved:\n","        print('iterationNo = ', iterationNo)\n","        dfWeights, dfRanks = None, None\n","        if featureNames.shape[0] == 0:\n","          dfWeights = pd.DataFrame(np.array(weightsOfMethods[method]))\n","          dfRanks = pd.DataFrame(np.array(ranksOfMethods[method]))\n","        else:\n","          dfWeights = pd.DataFrame(np.array(weightsOfMethods[method]), columns=featureNames)\n","          dfRanks = pd.DataFrame(np.array(ranksOfMethods[method]), columns=featureNames)\n","\n","        dfWeights = dfWeights.rename_axis('Iterations', axis='index').rename_axis('Features', axis = 'columns')\n","        dfWeights.to_csv(folderPath + '/Gene/Weights/{}/{}.csv'.format(dName,method), sep=\",\")\n","\n","        dfRanks = dfRanks.rename_axis('Iterations', axis='index').rename_axis('Features', axis = 'columns')\n","        dfRanks.to_csv(folderPath + '/Gene/Ranks/{}/{}.csv'.format(dName,method), sep=\",\")\n","      \n","      elif iterationNo > minItrSaved:\n","        print('iterationNo = ', iterationNo)\n","        newDfWeights = pd.DataFrame(np.array([weight]), index=[iterationNo])\n","        newDfRanks = pd.DataFrame(np.array([ranked]), index=[iterationNo])\n","\n","        newDfWeights.to_csv(folderPath + '/Gene/Weights/{}/{}.csv'.format(dName,method), sep=\",\", mode='a', header = False)\n","        newDfRanks.to_csv(folderPath + '/Gene/Ranks/{}/{}.csv'.format(dName,method), sep=\",\", mode='a', header = False) \n","\n","  # saving avg weigts of all methods\n","  avgWeightsOfMethods ={}\n","  for method in methods:\n","    weightsOfMethods[method] = np.array(weightsOfMethods[method])\n","    avgWeights = np.sum(weightsOfMethods[method],axis=0)/numOfsplits\n","    avgWeightsOfMethods[method]=avgWeights\n","\n","  dfAvgWeights = None\n","  if path.exists(folderPath+ '/Gene/Weights/{}/avgWeights.csv'.format(dName)):\n","    dfAvgWeights = pd.read_csv(folderPath + '/Gene/Weights/{}/avgWeights.csv'.format(dName), sep=\",\", index_col=\"Methods\", header=0)\n","    for method in methods:\n","      dfAvgWeights.loc[method] = avgWeightsOfMethods[method]\n","  else:\n","    if featureNames.shape[0]==0:\n","      dfAvgWeights= pd.DataFrame.from_dict(avgWeightsOfMethods, orient='index')\n","    else:\n","      dfAvgWeights= pd.DataFrame.from_dict(avgWeightsOfMethods, orient='index', columns=featureNames)\n","  dfAvgWeights = dfAvgWeights.rename_axis('Methods', axis='index').rename_axis('Features', axis = 'columns')\n","  dfAvgWeights.to_csv(folderPath + '/Gene/Weights/{}/avgWeights.csv'.format(dName), sep=\",\")\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZXxOmKdB8z49"},"source":["### Run classification"]},{"cell_type":"code","metadata":{"id":"dyD3lvqC6hbp"},"source":["def run_and_save_output(methods, dName, datasetType='gene', numOfFolds = 10, numOfRepeats = 5, noisePercentage=0, numOfPoints = 30, firstEndPoint = 500, secondEndPoint = 2000, featureRange = np.array([])):\n","  global randomState, folderPath\n","  # print('in run_and_save_output datasetType= ',datasetType)\n","  isNoisy = noisePercentage > 0\n","\n","  totalSvmScores = 0\n","  totalKnnScores = 0\n","\n","  dataset_dict =   load_dataset(dName) if (datasetType == 'normal') else load_gene_dataset(dName) \n","  # print('datasetType= ',datasetType)\n","\n","  X = dataset_dict['attributes']\n","  Y = dataset_dict['target']\n","  categoricalX = dataset_dict['categoricalX'] \n","\n","  # numOfPointsFirstRange = int(round(numOfPoints*(2/3)))\n","  # numOfPointsSecondRange = int(numOfPoints - numOfPointsFirstRange)\n","  # firstEndPoint = min(X.shape[1], firstEndPoint)\n","  # firstRange = np.linspace(1, firstEndPoint, numOfPointsFirstRange, dtype=int)\n","\n","  # secondEndPoint = min(X.shape[1], secondEndPoint)\n","  # secondRangeStart = firstEndPoint + (secondEndPoint - firstEndPoint)/numOfPointsSecondRange \n","  # secondRange = np.linspace(secondRangeStart, secondEndPoint, numOfPointsSecondRange , dtype=int)\n","\n","  # featureRange = np.unique(np.append(firstRange, secondRange))\n","  # print(featureRange)\n","\n","  tempFeatureRange = np.array([1,5,10,20,40,60,80,100,120,140,160,180,200,220,250,280,300,350,400,450,500,750,1000,1250,1500,2000])\n","  featureRange = featureRange if len(featureRange) != 0 else tempFeatureRange\n","\n","  # leaveOneOut = LeaveOneOut()\n","  # numOfsplits = leaveOneOut.get_n_splits(X)\n","  repeatedKFold = RepeatedKFold(n_splits = numOfFolds, n_repeats = numOfRepeats, random_state=randomState)\n","  numOfsplits = repeatedKFold.get_n_splits(X)\n","  YActual = np.array([])\n","  YPredSVMDict = {}\n","  YPredKNNDict = {}\n","  YCollected= False\n","\n","  svmScoresList = []\n","  knnScoresList = []\n","  \n","  for method in methods:\n","    df = pd.read_csv(folderPath  +  '/Gene/Ranks/{}/{}.csv'.format(dName, method), sep=\",\",index_col=\"Iterations\", header=0)\n","    featureNames = df.columns.to_numpy() #featureNames in an array\n","    ranks = df.to_numpy() #feature ranks of each iteration in each rows of ranks array\n","\n","    svmScores = 0\n","    knnScores = 0\n","    itrationNo = -1\n","    for trainIndex, testIndex in repeatedKFold.split(X):\n","      itrationNo += 1\n","      # print(\"TRAIN:\", trainIndex, \"TEST:\", testIndex)\n","      XTrain, XTest, YTrain, YTest = X[trainIndex], X[testIndex], Y[trainIndex], Y[testIndex]\n","      if not YCollected:\n","        YActual = np.append(YActual, YTest) \n","\n","      # Here if we estimate best K for KNN classification using 10 fold stratified cross validation, it will help us to do comparison more fairly.\n","      # Here If we estimate best kernelWidth sigma for I-Relief using 10 fold stratified cross validation, it will help us to do comparison more fairly. \n","      # Here If we estimate best K(i.e. number of nearest hits and misses) for Relief-F using 10 fold stratified cross validation, it will help us to do comparison more fairly. \n","\n","\n","      # performing feture selection \n","      # Using kernelwidth sigma = 5 and number of iterations = 20 and convergence_threshold = 1e-5 for I-Relief-1 and I-Relief-2\n","      # Using number of nearest hits and misses i.e. K = 10 for ReliefF and also using all the training data for Relief and ReliefF\n","      # Using number of nearest Neighbours k = 7 and overlapping_threshold = 3 for RFS\n","      # print('Training data size: training examples = {}, features= {}'.format(XTrain.shape[0], XTrain.shape[1]))\n","\n","      svmYPred, svmScore = classification(XTrain, XTest, YTrain, YTest, ranks[itrationNo], algo = 'SVM', featureRange = featureRange, datasetName = dName, kernel='linear', degree=3)\n","      knnYPred, knnScore = classification(XTrain, XTest, YTrain, YTest, ranks[itrationNo], algo = 'KNN', featureRange = featureRange, datasetName = dName, Num_neighbors=7)\n","      svmScores += svmScore\n","      knnScores += knnScore  \n","      if method not in YPredSVMDict:\n","        YPredSVMDict[method] = svmYPred\n","        YPredKNNDict[method] = knnYPred\n","      else:\n","        YPredSVMDict[method] = np.append(YPredSVMDict[method], svmYPred, axis = 1)\n","        YPredKNNDict[method] = np.append(YPredKNNDict[method], knnYPred, axis = 1)\n","    \n","    svmScoresList.append(svmScores)  #summation of svmScores for each method is appending in svmScoresList\n","    knnScoresList.append(knnScores)  #summation of knnScores for each method is appending in knnScoresList\n","    YCollected= True\n","    \n","\n","  avgSvmScores = np.array(svmScoresList)/numOfsplits\n","  avgKnnScores = np.array(knnScoresList)/numOfsplits\n","\n","  classifiers = np.array(['KNN', 'SVM'])\n","  # print(classifiers.shape, methods.shape)\n","  # print(svmScores.shape, knnScores.shape)\n","  allScores = np.concatenate((avgKnnScores, avgSvmScores), axis=0)\n","  index = pd.MultiIndex.from_product([classifiers, methods], names=['Classifier', 'Method'])\n","\n","  filePath = \"\"\n","  if isNoisy:\n","    filePath = folderPath  +  '/Gene/ClassificationAccuracy/{}_withNoise.csv'.format(dName)\n","  else:\n","    filePath = folderPath  +  '/Gene/ClassificationAccuracy/{}_withoutNoise.csv'.format(dName)\n","  \n","  if path.exists(filePath):\n","    df = pd.read_csv(filePath, sep=\",\", index_col=['Classifier','Method'])\n","    for i in range(index.to_series().nunique()):\n","      df.loc[index[i],:] = allScores[i] \n","  else:\n","    df = pd.DataFrame(allScores, index=index, columns=featureRange)\n","    df = df.rename_axis('#features', axis='columns')\n","\n","  print(\"Dataset Name :\", dName)    \n","  print(df)\n","  df.to_csv(filePath, sep=\",\")\n","  \n","  # Writing precision, recall, f1-score, support to the file\n","  performanceReport(methods, YActual, YPredSVMDict, featureRange, dName, isNoisy, 'SVM')\n","  performanceReport(methods, YActual, YPredKNNDict, featureRange, dName, isNoisy, 'KNN')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOTjQFmTeGzl"},"source":["# def run_and_save_output(dName, datasetType='gene', noisePercentage=0, numOfPoints = 30, firstEndPoint = 500, secondEndPoint = 2000):\n","#   global randomState\n","#   # print('in run_and_save_output datasetType= ',datasetType)\n","#   isNoisy = noisePercentage > 0\n","\n","#   totalSvmScores = 0\n","#   totalKnnScores = 0\n","\n","#   dataset_dict =   load_dataset(dName) if (datasetType == 'normal') else load_gene_dataset(dName) \n","#   # print('datasetType= ',datasetType)\n","\n","#   X = dataset_dict['attributes']\n","#   Y = dataset_dict['target']\n","#   categoricalX = dataset_dict['categoricalX'] \n","  \n","#   # leaveOneOut = LeaveOneOut()\n","#   # numOfsplits = leaveOneOut.get_n_splits(X)\n","#   repeatedKFold = KFold(n_splits=10)\n","#   numOfsplits = repeatedKFold.get_n_splits(X)\n","#   YActual = np.array([])\n","#   YPredSVMDict = {}\n","#   YPredKNNDict = {}\n","\n","#   for trainIndex, testIndex in repeatedKFold.split(X):\n","#     # print(\"TRAIN:\", trainIndex, \"TEST:\", testIndex)\n","#     XTrain, XTest, YTrain, YTest = X[trainIndex], X[testIndex], Y[trainIndex], Y[testIndex]\n","#     YActual = np.append(YActual, YTest) \n","\n","#     # Here if we estimate best K for KNN classification using 10 fold stratified cross validation, it will help us to do comparison more fairly.\n","#     # Here If we estimate best kernelWidth sigma for I-Relief using 10 fold stratified cross validation, it will help us to do comparison more fairly. \n","#     # Here If we estimate best K(i.e. number of nearest hits and misses) for Relief-F using 10 fold stratified cross validation, it will help us to do comparison more fairly. \n","\n","\n","#     # performing feture selection \n","#     # Using kernelwidth sigma = 3 and number of iterations = 20 and convergence_threshold = 1e-5 for I-Relief-1 and I-Relief-2\n","#     # Using number of nearest hits and misses i.e. K = 10 for ReliefF and also using all the training data for Relief and ReliefF\n","#     # Using number of nearest Neighbours k = 7 and overlapping_threshold = 3 for RFS\n","#     # print('Training data size: training examples = {}, features= {}'.format(XTrain.shape[0], XTrain.shape[1]))\n","    \n","#     # Feature Selection\n","#     cache = feature_selection(XTrain, YTrain, K_RFS = 7, K_ReliefF = 10, theta_RFS = 3, numUpdates='all', numOfIterations = 20, prior='empirical', categoricalX='off', kernelWidth = 5, theta_IRelief = 1e-5)\n","    \n","#     numOfPointsFirstRange = int(round(numOfPoints*(2/3)))\n","#     numOfPointsSecondRange = int(numOfPoints - numOfPointsFirstRange)\n","#     firstEndPoint = min(XTrain.shape[1], firstEndPoint)\n","#     firstRange = np.linspace(1, firstEndPoint, numOfPointsFirstRange, dtype=int)\n","\n","#     secondEndPoint = min(XTrain.shape[1], secondEndPoint)\n","#     secondRangeStart = firstEndPoint + (secondEndPoint - firstEndPoint)/numOfPointsSecondRange \n","#     secondRange = np.linspace(secondRangeStart, secondEndPoint, numOfPointsSecondRange , dtype=int)\n","\n","#     featureRange = np.unique(np.append(firstRange, secondRange))\n","#     print(featureRange)\n","\n","#     svmScores = []\n","#     knnScores = []\n","#     methods = np.array([])\n","\n","#     for method, rankWeight in cache.items():\n","#       ranked, weight = rankWeight\n","#       # print(method, ranked[:min(XTrain.shape[1], 1000)])\n","#       methods = np.append(methods, method)\n","\n","#       svmYPred, svmScore = classification(XTrain, XTest, YTrain, YTest, ranked, algo = 'SVM', featureRange = featureRange, datasetName = dName, kernel='linear', degree=3)\n","#       knnYPred, knnScore = classification(XTrain, XTest, YTrain, YTest, ranked, algo = 'KNN', featureRange = featureRange, datasetName = dName, Num_neighbors=7)\n","#       svmScores.append(svmScore)\n","#       knnScores.append(knnScore)\n","\n","#       if method not in YPredSVMDict:\n","#         YPredSVMDict[method] = svmYPred\n","#         YPredKNNDict[method] = knnYPred\n","#       else:\n","#         YPredSVMDict[method] = np.append(YPredSVMDict[method], svmYPred, axis = 1)\n","#         YPredKNNDict[method] = np.append(YPredKNNDict[method], knnYPred, axis = 1)\n","\n","#     svmScores = np.array(svmScores)\n","#     knnScores = np.array(knnScores)\n","#     totalSvmScores = totalSvmScores + svmScores\n","#     totalKnnScores = totalKnnScores + knnScores\n","\n","#   avgSvmScores = totalSvmScores/numOfsplits\n","#   avgKnnScores = totalKnnScores/numOfsplits\n","\n","#   classifiers = np.array(['KNN', 'SVM'])\n","#   # print(classifiers.shape, methods.shape)\n","#   # print(svmScores.shape, knnScores.shape)\n","#   index = pd.MultiIndex.from_product([classifiers, methods], names=['Classifier', 'Method'])\n","#   df = pd.DataFrame(np.concatenate((avgKnnScores, avgSvmScores), axis=0), index=index, columns=featureRange)\n","#   df = df.rename_axis('#features', axis='columns')\n","#   print(\"Dataset Name :\", dName)\n","#   print(df)\n","#   if isNoisy:\n","#     df.to_csv(folderPath  +  '/Gene/ClassificationAccuracy/{}_withNoise.csv'.format(dName), sep=\",\")\n","#   else:\n","#     df.to_csv(folderPath  +  '/Gene/ClassificationAccuracy/{}_withoutNoise.csv'.format(dName), sep=\",\")\n","\n","#   # Writing precision, recall, f1-score, support to the file\n","#   performanceReport(methods, YActual, YPredSVMDict, featureRange, dName, isNoisy, 'SVM')\n","#   performanceReport(methods, YActual, YPredKNNDict, featureRange, dName, isNoisy, 'KNN')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2fVUW7oUJgH"},"source":["##Plotting"]},{"cell_type":"code","metadata":{"id":"iYboxdTlUL5e"},"source":["def plotting(mehtods, dName):\n","  global randomState, folderPath, plotStyles\n","\n","  # extension = ['withoutNoise', 'withNoise']\n","  extension = ['withoutNoise']\n","  plt.figure(figsize=(20,10*len(extension)))\n","\n","  for i in range(len(extension)):\n","    df = pd.read_csv(folderPath  +  '/Gene/ClassificationAccuracy/{}_{}.csv'.format(dName, extension[i]), sep=\",\", index_col=['Classifier','Method'])\n","    noOfFeatures = np.array(df.columns, dtype=int)\n","    classifiers = np.unique(df.index.get_level_values('Classifier'))\n","    noOfClassifiers = len(classifiers)\n","    # methods = np.unique(df.index.get_level_values('Method'))\n","    noOfMethods = len(methods)\n","    lines = []\n","\n","    for j in range(noOfClassifiers):\n","      plt.subplot(len(extension), noOfClassifiers, (noOfClassifiers * (i)) + j + 1).set_title('Dataset: {} {}, Classifier : {} '.format(dName, extension[i], classifiers[j]), fontsize=20)\n","      plt.ylabel('Accuracy', fontsize=20)\n","      plt.xlabel('#features', fontsize=20)\n","\n","      takenMethods =methods\n","      # for k in range(noOfMethods):\n","      #     takenMethods.append(methods[k])\n","\n","      labels = []    \n","      for k in range(len(takenMethods)):\n","        label=takenMethods[k]\n","        if label == 'Modified_Overlapping_MultiSurf':\n","          label = 'mMultiSURF'\n","        # elif label == 'ReBATE_MultiSURF':\n","        #   label = 'MultiSURF'\n","        if label.find('ReBATE_')!=-1:\n","          label= label.replace('ReBATE_','')\n","        labels.append(label)\n","\n","        line,  = plt.plot(np.arange(len(noOfFeatures)), df.loc[(classifiers[j], takenMethods[k])], color=plotStyles['color'][k], linestyle=plotStyles['lineStyle'], \n","                  marker=plotStyles['markers'][k], markersize=plotStyles['markerSize'], label=label)\n","        lines.append(line)\n","          \n","      plt.xticks(np.arange(len(noOfFeatures)), noOfFeatures,  rotation='vertical')\n","      \n","      # # locs, labels = plt.xticks()\n","      # every_nth = 3\n","      # for n, label in enumerate(plt.xticks()):\n","      #   if n % every_nth != 0:\n","      #     label.visible = False\n","\n","      # locs, lbls = plt.xticks()\n","      # lbls[1].visible = False\n","      # plt.xticks(locs, lbls) \n","      \n","      plt.legend(lines, labels, prop={'size': 18})\n","        \n","  plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFLcRZONGTur"},"source":["# Saving Plots"]},{"cell_type":"code","metadata":{"id":"-CUPG30OGguJ"},"source":["def save_plot(methods, dName, fileExtension):\n","  global randomState, folderPath, plotStyles\n","\n","  # extension = ['withoutNoise', 'withNoise']\n","  extension = ['withoutNoise']\n","  plt.figure(figsize=(10,10))\n","\n","  for i in range(len(extension)):\n","    df = pd.read_csv(folderPath + '/Gene/ClassificationAccuracy/{}_{}.csv'.format(dName, extension[i]), sep=\",\", index_col=['Classifier','Method'])\n","    noOfFeatures = np.array(df.columns, dtype=int)\n","    classifiers = np.unique(df.index.get_level_values('Classifier'))\n","    noOfClassifiers = len(classifiers)\n","    # methods = np.unique(df.index.get_level_values('Method'))\n","    noOfMethods = len(methods)\n","    lines = []\n","\n","    for j in range(noOfClassifiers):\n","      plt.clf()\n","      plt.title('Dataset: {} {}, Classifier : {} '.format(dName, extension[i], classifiers[j]), fontsize=20)\n","      plt.ylabel('Accuracy', fontsize=20)\n","      plt.xlabel('#features', fontsize=20)\n","      \n","      takenMethods =methods\n","      # for k in range(noOfMethods):\n","      #     takenMethods.append(methods[k])\n","\n","\n","      labels = []    \n","      for k in range(len(takenMethods)):\n","        label=takenMethods[k]\n","        if label == 'Modified_Overlapping_MultiSurf':\n","          label = 'mMultiSURF'\n","        # elif label == 'ReBATE_MultiSURF':\n","        #   label = 'MultiSURF'\n","        if label.find('ReBATE_')!=-1:\n","          label= label.replace('ReBATE_','')\n","        labels.append(label)\n","\n","        line,  = plt.plot(np.arange(len(noOfFeatures)), df.loc[(classifiers[j], takenMethods[k])], color=plotStyles['color'][k], linestyle=plotStyles['lineStyle'], \n","                  marker=plotStyles['markers'][k], markersize=plotStyles['markerSize'], label=label)\n","        lines.append(line)\n","\n","      plt.xticks(np.arange(len(noOfFeatures)), noOfFeatures,  rotation='vertical')    \n","      plt.legend(lines, labels, prop={'size': 18})  \n","      plt.savefig(folderPath + '/Gene/Plots/ClassificationAccuracy/{}/{}/{}.{}'.format(extension[i], classifiers[j], dName, fileExtension)) \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AnQjlqJuyZyQ"},"source":["### Methods to Run"]},{"cell_type":"code","metadata":{"id":"t6V-q1POydG3"},"source":["# methods = np.array(['RFS', 'Relief', 'ReliefF', 'IRelief', 'IRelief2', 'MultiSurf', 'Modified_MultiSurf', 'Overlapping_MultiSurf',\n","#            'Modified_Overlapping_MultiSurf', 'Overlapping_ReliefF', 'ReBATE_ReliefF', 'ReBATE_SURF', 'ReBATE_SURFstar', \n","#            'ReBATE_MultiSURFstar', 'ReBATE_MultiSURF'])\n","# methods = np.array(['Modified_Overlapping_MultiSurf', 'ReliefF',  'ReBATE_ReliefF', 'ReBATE_SURF', 'ReBATE_SURFstar', \n","#            'ReBATE_MultiSURFstar', 'ReBATE_MultiSURF', 'Overlapping_MultiSurf'])\n","methods = np.array(['Modified_Overlapping_MultiSurf', 'ReliefF', 'ReBATE_MultiSURF','ReBATE_SURF' ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iXn5vNIZSvaB"},"source":["###CNS(7129/60/2) "]},{"cell_type":"code","metadata":{"id":"fEuFTrcUSx_t"},"source":["# run_and_save_feature_weights(methods, dName = 'cns', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'cns', datasetType = 'gene')\n","plotting(methods, 'cns')\n","save_plot(methods, 'cns', 'svg')\n","save_plot(methods, 'cns', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PXlyYWkSClyw"},"source":["### Colon(2000/62/2)"]},{"cell_type":"code","metadata":{"id":"e9rjsBz0Cmrj"},"source":["# run_and_save_feature_weights(methods, dName = 'colon', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'colon', datasetType = 'gene')\n","plotting(methods, 'colon')\n","save_plot(methods, 'colon', 'svg')\n","save_plot(methods, 'colon', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0nvhkqEjUkAm"},"source":["###lymphoma (4026/66/3) "]},{"cell_type":"code","metadata":{"id":"Vmond1wRUkAq"},"source":["# run_and_save_feature_weights(methods, dName = 'lymphoma', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'lymphoma', datasetType = 'gene')\n","plotting(methods, 'lymphoma')\n","save_plot(methods, 'lymphoma', 'svg')\n","save_plot(methods, 'lymphoma', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Fn5XwkVUkLZ"},"source":["###mll(12582/72/3)"]},{"cell_type":"code","metadata":{"id":"bSvJ8LpFUkLa"},"source":["# run_and_save_feature_weights(methods, dName = 'mll', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'mll', datasetType = 'gene')\n","plotting(methods, 'mll')\n","save_plot(methods, 'mll', 'svg')\n","save_plot(methods, 'mll', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMmw-mxyUknZ"},"source":["###srbct(2308/83/4)"]},{"cell_type":"code","metadata":{"id":"1clRjcYMUknb"},"source":["# run_and_save_feature_weights(methods, dName = 'srbct', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'srbct', datasetType = 'gene')\n","plotting(methods, 'srbct')\n","save_plot(methods, 'srbct', 'svg')\n","save_plot(methods, 'srbct', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okmFNnsHUOOz"},"source":["###Leukemia(7129/72/2)"]},{"cell_type":"code","metadata":{"id":"t3v1UX_9UFv_"},"source":["# run_and_save_feature_weights(methods, dName = 'leukemia', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'leukemia', datasetType = 'gene')\n","plotting(methods, 'leukemia')\n","save_plot(methods, 'leukemia', 'svg')\n","save_plot(methods, 'leukemia', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JSwNd5kWUi6K"},"source":["###Leukemia3c(7129/72/3) "]},{"cell_type":"code","metadata":{"id":"VBzowOnzUi6d"},"source":["# run_and_save_feature_weights(methods, dName = 'leukemia3c', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'leukemia3c', datasetType = 'gene')\n","plotting(methods, 'leukemia3c')\n","save_plot(methods, 'leukemia3c', 'svg')\n","save_plot(methods, 'leukemia3c', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dwH-CoB0UjNo"},"source":["###Leukemia4c(imbalanced)  (7129/72/4) "]},{"cell_type":"code","metadata":{"id":"XTI3Cn4hUjNq"},"source":["# run_and_save_feature_weights(methods, dName = 'leukemia4c', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'leukemia4c', datasetType = 'gene')\n","plotting(methods, 'leukemia4c')\n","save_plot(methods, 'leukemia4c', 'svg')\n","save_plot(methods, 'leukemia4c', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVXhQv_PV5rX"},"source":["# Gene Datasets Large"]},{"cell_type":"code","metadata":{"id":"ZhyNaoTxV-ua"},"source":["featureRange = np.array([1,5,10,20,50,80,100,120,150,180,200,220,250,300,350,400,450,500,750,1000,1250,1500,2000, 2500, 3000,3500, 4000, 4500, 5000])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPaCQ9gLKX_g"},"source":["### merged_GDS3341  (30803 /41/2)"]},{"cell_type":"code","metadata":{"id":"lyOEWdGpPCT5"},"source":["# run_and_save_feature_weights(methods, dName = 'merged_GDS3341', datasetType = 'gene')\n","run_and_save_output(methods, dName = 'merged_GDS3341', datasetType = 'gene', featureRange =featureRange)\n","plotting(methods, 'merged_GDS3341')\n","save_plot(methods, 'merged_GDS3341', 'svg')\n","save_plot(methods, 'merged_GDS3341', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MI1ClnXfKg__"},"source":["### merged_GDS3610  (14060 /28/2)"]},{"cell_type":"code","metadata":{"id":"B-UT37WQPELk"},"source":["# run_and_save_feature_weights(methods, dName = 'merged_GDS3610', datasetType = 'gene')\n","run_and_save_output(methods, dName = 'merged_GDS3610', datasetType = 'gene', featureRange =featureRange)\n","plotting(methods,'merged_GDS3610')\n","save_plot(methods,'merged_GDS3610', 'svg')\n","save_plot(methods,'merged_GDS3610', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hK8q1yxwKrN9"},"source":["### merged_GDS3858  (14051  /34/2)"]},{"cell_type":"code","metadata":{"id":"BbW1zqHBPGq4"},"source":["# run_and_save_feature_weights(methods, dName = 'merged_GDS3858', datasetType = 'gene')\n","run_and_save_output(methods, dName = 'merged_GDS3858', datasetType = 'gene', featureRange =featureRange)\n","plotting(methods, 'merged_GDS3858')\n","save_plot(methods, 'merged_GDS3858', 'svg')\n","save_plot(methods, 'merged_GDS3858', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"njfah6WVKuHG"},"source":["### merged_GDS4167  (14060  /52/2)"]},{"cell_type":"code","metadata":{"id":"5jHhVX_rPIHd"},"source":["# run_and_save_feature_weights(methods, dName = 'merged_GDS4167', datasetType = 'gene')\n","run_and_save_output(methods, dName = 'merged_GDS4167', datasetType = 'gene', featureRange =featureRange)\n","plotting(methods,'merged_GDS4167')\n","save_plot(methods,'merged_GDS4167', 'svg')\n","save_plot(methods,'merged_GDS4167', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TdbyLvLqKyDu"},"source":["###merged_GDS4168  (16464 /52/2)"]},{"cell_type":"code","metadata":{"id":"2DngMCWpPJRJ"},"source":["# run_and_save_feature_weights(methods, dName = 'merged_GDS4168', datasetType = 'gene')\n","run_and_save_output(methods, dName = 'merged_GDS4168', datasetType = 'gene', featureRange =featureRange)\n","plotting(methods,'merged_GDS4168')\n","save_plot(methods,'merged_GDS4168', 'svg')\n","save_plot(methods,'merged_GDS4168', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Qe9OIm3K6bR"},"source":["### merged_GDS4824  (30803 /21/2)"]},{"cell_type":"code","metadata":{"id":"9-f7Jck2PKk7"},"source":["# run_and_save_feature_weights(methods, dName = 'merged_GDS4824', datasetType = 'gene')\n","run_and_save_output(methods, dName = 'merged_GDS4824', datasetType = 'gene', featureRange =featureRange)\n","plotting(methods, 'merged_GDS4824')\n","save_plot(methods, 'merged_GDS4824', 'svg')\n","save_plot(methods, 'merged_GDS4824', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"soEnFoCSK9LD"},"source":["### merged_GDS5306  (32389  /38/2)"]},{"cell_type":"code","metadata":{"id":"ElMsSJLtPLl-"},"source":["featureRange = np.array([1,5,10,20,40,60,100,120,160,200,220,300,400,450,500,750,1000,1250,1500,2000])\n","# run_and_save_feature_weights(methods, dName = 'merged_GDS5306', datasetType = 'gene')\n","run_and_save_output(methods, dName = 'merged_GDS5306', datasetType = 'gene', featureRange= featureRange)\n","plotting(methods, 'merged_GDS5306')\n","save_plot(methods, 'merged_GDS5306', 'svg')\n","save_plot(methods, 'merged_GDS5306', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-CqbIojJntl"},"source":["# Unfinished run"]},{"cell_type":"markdown","metadata":{"id":"eXY3esO6Kn-R"},"source":["### merged_GDS3837  (30779 /120/2)\n","\n"]},{"cell_type":"code","metadata":{"id":"fatq_9xQPFKw"},"source":["# run_and_save_feature_weights(methods, dName = 'merged_GDS3837', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'merged_GDS3837', datasetType = 'gene')\n","# plotting('merged_GDS3837')\n","# save_plot('merged_GDS3837', 'svg')\n","# save_plot('merged_GDS3837', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_3YNF5YPK4fj"},"source":["### merged_GDS4431  (30803  /146/2)"]},{"cell_type":"code","metadata":{"id":"KCuDH0WdPKMA"},"source":["# run_and_save_feature_weights(methods, dName = 'merged_GDS4431', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'merged_GDS4431', datasetType = 'gene')\n","# plotting('merged_GDS4431')\n","# save_plot('merged_GDS4431', 'svg')\n","# save_plot('merged_GDS4431', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Ri41mfQK_5S"},"source":["### merged_GSE106291  (21402/235/2)"]},{"cell_type":"code","metadata":{"id":"KoawWLorPMj8"},"source":["# run_and_save_feature_weights(methods, dName = 'merged_GSE106291', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'merged_GSE106291', datasetType = 'gene')\n","# plotting('merged_GSE106291')\n","# save_plot('merged_GSE106291', 'svg')\n","# save_plot('merged_GSE106291', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uoZ1Skb4UjZ2"},"source":["###lung2(imbalanced)(12600/203/_) "]},{"cell_type":"code","metadata":{"id":"goRDyZncUjZ5"},"source":["# run_and_save_feature_weights(methods, dName = 'lung2', datasetType = 'gene')\n","# run_and_save_output(methods, dName = 'lung2', datasetType = 'gene')\n","# plotting('lung2')\n","# save_plot('lung2', 'svg')\n","# save_plot('lung2', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wGjTUcU_UkWd"},"source":["###ovarian(15154/253/_) "]},{"cell_type":"code","metadata":{"id":"EyDzJfiWUkWe"},"source":["# run_and_save_feature_weights(methods, dName = 'ovarian', datasetType = 'gene')\n","# print(\"feature selection done\")\n","# run_and_save_output(methods, dName = 'ovarian', datasetType = 'gene')\n","# plotting('ovarian')\n","# save_plot('ovarian', 'svg')\n","# save_plot('ovarian', 'png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VVRrY8fmVQH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KJZdWQUEjBzR"},"source":["## For Statisctical Analysis Purpose"]},{"cell_type":"code","metadata":{"id":"s5tKTYtYjGVd"},"source":["# datasets = ['cns', 'colon', 'lymphoma', 'leukemia3c', 'leukemia4c', 'leukemia', 'merged_GDS3341', 'merged_GDS5306']\n","# methods =  np.array(['Modified_Overlapping_MultiSurf', 'ReliefF', 'ReBATE_MultiSURF','ReBATE_SURF' ])\n","# extension = ['withoutNoise']\n","\n","# for dName in datasets:\n","#   statisticalAnalysisPath = \"/content/drive/MyDrive/Thesis/Results/Statistical_Analysis/Accuracy/GeneDataset/{}.csv\".format(dName)\n","#   for i in range(len(extension)):\n","#     df = pd.read_csv(folderPath  +  '/Gene/ClassificationAccuracy/{}_{}.csv'.format(dName, extension[i]), sep=\",\", index_col=['Classifier','Method'])\n","#     noOfFeatures = np.array(df.columns, dtype=int) \n","\n","#     scoreCollection = []\n","#     labels = []\n","#     for k in range(len(methods)):\n","#       score = df.loc[('KNN', methods[k])]\n","#       scoreCollection.append(score.to_numpy())\n","      \n","#       label = methods[k]\n","#       if label == 'Modified_Overlapping_MultiSurf':\n","#         label = 'mMultiSURF'\n","#       if label.find('ReBATE_')!=-1:\n","#         label= label.replace('ReBATE_','')\n","#       labels.append(label)\n","    \n","#     # print(scoreCollection)\n","#     # print(labels)\n","\n","#     newDf = pd.DataFrame(np.array(scoreCollection), index=np.array(labels), columns=noOfFeatures)\n","#     newDf.to_csv(statisticalAnalysisPath, sep=',')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kU1KMO8BqXaS"},"source":["# datasets = ['cns', 'colon', 'lymphoma', 'leukemia3c', 'leukemia4c', 'leukemia', 'merged_GDS5306']\n","# methods =  np.array(['Modified_Overlapping_MultiSurf' ])\n","# extension = ['withoutNoise']\n","# deletedFeatureRange = np.array(['80', '140','180', '250', '280', '350'])\n","\n","# scoreCollection = []\n","\n","# for dName in datasets:\n","#   statisticalAnalysisPath = \"/content/drive/MyDrive/Thesis/Results/Statistical_Analysis/accuracyOfAllGeneDatasets.csv\"\n","#   for i in range(len(extension)):\n","#     df = pd.read_csv(folderPath  +  '/Gene/ClassificationAccuracy/{}_{}.csv'.format(dName, extension[i]), sep=\",\", index_col=['Classifier','Method'])\n","#     score = df.loc[('KNN', 'Modified_Overlapping_MultiSurf')]\n","#     # print(score)\n","#     score = score.drop(labels=deletedFeatureRange, errors='ignore')\n","#     noOfFeatures = np.array(score.index, dtype=int)\n","#     scoreCollection.append(score)\n","\n","# newDf = pd.DataFrame(np.array(scoreCollection), index=np.array(datasets), columns=noOfFeatures)\n","# newDf.to_csv(statisticalAnalysisPath, sep=',')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0O5aqjL01xT"},"source":[""],"execution_count":null,"outputs":[]}]}